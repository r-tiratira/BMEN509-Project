{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import system libs\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pathlib\n",
    "import itertools\n",
    "from glob import glob\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "\n",
    "# import data handling tools\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.morphology import label\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "\n",
    "# import Deep learning Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model, load_model, save_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Input, Activation, BatchNormalization, Dropout, Lambda, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print ('modules loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create dataframe\n",
    "def create_df(data_dir):\n",
    "    images_paths = []\n",
    "    masks_paths = glob(f'{data_dir}/*/*_mask*')\n",
    "\n",
    "    for i in masks_paths:\n",
    "        images_paths.append(i.replace('_mask', ''))\n",
    "\n",
    "    df = pd.DataFrame(data= {'images_paths': images_paths, 'masks_paths': masks_paths})\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to split dataframe into train, valid, test\n",
    "def split_df(df):\n",
    "    # create train_df\n",
    "    train_df, dummy_df = train_test_split(df, train_size= 0.8)\n",
    "\n",
    "    # create valid_df and test_df\n",
    "    valid_df, test_df = train_test_split(dummy_df, train_size= 0.5)\n",
    "\n",
    "    return train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gens(df, aug_dict):\n",
    "    img_size = (256, 256)\n",
    "    batch_size = 40\n",
    "\n",
    "\n",
    "    img_gen = ImageDataGenerator(**aug_dict)\n",
    "    msk_gen = ImageDataGenerator(**aug_dict)\n",
    "\n",
    "    # Create general generator\n",
    "    image_gen = img_gen.flow_from_dataframe(df, x_col='images_paths', class_mode=None, color_mode='rgb', target_size=img_size,\n",
    "                                            batch_size=batch_size, save_to_dir=None, save_prefix='image', seed=1)\n",
    "\n",
    "    mask_gen = msk_gen.flow_from_dataframe(df, x_col='masks_paths', class_mode=None, color_mode='grayscale', target_size=img_size,\n",
    "                                            batch_size=batch_size, save_to_dir=None, save_prefix= 'mask', seed=1)\n",
    "\n",
    "    gen = zip(image_gen, mask_gen)\n",
    "\n",
    "    for (img, msk) in gen:\n",
    "        img = img / 255\n",
    "        msk = msk / 255\n",
    "        msk[msk > 0.5] = 1\n",
    "        msk[msk <= 0.5] = 0\n",
    "\n",
    "        yield (img, msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet(input_size=(256, 256, 3)):\n",
    "    inputs = Input(input_size)\n",
    "\n",
    "    # First DownConvolution / Encoder Leg will begin, so start with Conv2D\n",
    "    conv1 = Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\")(inputs)\n",
    "    bn1 = Activation(\"relu\")(conv1)\n",
    "    conv1 = Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\")(bn1)\n",
    "    bn1 = BatchNormalization(axis=3)(conv1)\n",
    "    bn1 = Activation(\"relu\")(bn1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(bn1)\n",
    "\n",
    "    conv2 = Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\")(pool1)\n",
    "    bn2 = Activation(\"relu\")(conv2)\n",
    "    conv2 = Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\")(bn2)\n",
    "    bn2 = BatchNormalization(axis=3)(conv2)\n",
    "    bn2 = Activation(\"relu\")(bn2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(bn2)\n",
    "\n",
    "    conv3 = Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\")(pool2)\n",
    "    bn3 = Activation(\"relu\")(conv3)\n",
    "    conv3 = Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\")(bn3)\n",
    "    bn3 = BatchNormalization(axis=3)(conv3)\n",
    "    bn3 = Activation(\"relu\")(bn3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(bn3)\n",
    "\n",
    "    conv4 = Conv2D(filters=512, kernel_size=(3, 3), padding=\"same\")(pool3)\n",
    "    bn4 = Activation(\"relu\")(conv4)\n",
    "    conv4 = Conv2D(filters=512, kernel_size=(3, 3), padding=\"same\")(bn4)\n",
    "    bn4 = BatchNormalization(axis=3)(conv4)\n",
    "    bn4 = Activation(\"relu\")(bn4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(bn4)\n",
    "\n",
    "    conv5 = Conv2D(filters=1024, kernel_size=(3, 3), padding=\"same\")(pool4)\n",
    "    bn5 = Activation(\"relu\")(conv5)\n",
    "    conv5 = Conv2D(filters=1024, kernel_size=(3, 3), padding=\"same\")(bn5)\n",
    "    bn5 = BatchNormalization(axis=3)(conv5)\n",
    "    bn5 = Activation(\"relu\")(bn5)\n",
    "\n",
    "    \"\"\" Now UpConvolution / Decoder Leg will begin, so start with Conv2DTranspose\n",
    "    The gray arrows (in the above image) indicate the skip connections that concatenate the encoder feature map with the decoder, which helps the backward flow of gradients for improved training. \"\"\"\n",
    "    \"\"\" After every concatenation we again apply two consecutive regular convolutions so that the model can learn to assemble a more precise output \"\"\"\n",
    "\n",
    "    up6 = concatenate([Conv2DTranspose(512, kernel_size=(2, 2), strides=(2, 2), padding=\"same\")(bn5), conv4], axis=3)\n",
    "    conv6 = Conv2D(filters=512, kernel_size=(3, 3), padding=\"same\")(up6)\n",
    "    bn6 = Activation(\"relu\")(conv6)\n",
    "    conv6 = Conv2D(filters=512, kernel_size=(3, 3), padding=\"same\")(bn6)\n",
    "    bn6 = BatchNormalization(axis=3)(conv6)\n",
    "    bn6 = Activation(\"relu\")(bn6)\n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(256, kernel_size=(2, 2), strides=(2, 2), padding=\"same\")(bn6), conv3], axis=3)\n",
    "    conv7 = Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\")(up7)\n",
    "    bn7 = Activation(\"relu\")(conv7)\n",
    "    conv7 = Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\")(bn7)\n",
    "    bn7 = BatchNormalization(axis=3)(conv7)\n",
    "    bn7 = Activation(\"relu\")(bn7)\n",
    "\n",
    "    up8 = concatenate([Conv2DTranspose(128, kernel_size=(2, 2), strides=(2, 2), padding=\"same\")(bn7), conv2], axis=3)\n",
    "    conv8 = Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\")(up8)\n",
    "    bn8 = Activation(\"relu\")(conv8)\n",
    "    conv8 = Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\")(bn8)\n",
    "    bn8 = BatchNormalization(axis=3)(conv8)\n",
    "    bn8 = Activation(\"relu\")(bn8)\n",
    "\n",
    "    up9 = concatenate([Conv2DTranspose(64, kernel_size=(2, 2), strides=(2, 2), padding=\"same\")(bn8), conv1], axis=3)\n",
    "    conv9 = Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\")(up9)\n",
    "    bn9 = Activation(\"relu\")(conv9)\n",
    "    conv9 = Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\")(bn9)\n",
    "    bn9 = BatchNormalization(axis=3)(conv9)\n",
    "    bn9 = Activation(\"relu\")(bn9)\n",
    "\n",
    "    conv10 = Conv2D(filters=1, kernel_size=(1, 1), activation=\"sigmoid\")(bn9)\n",
    "\n",
    "    return Model(inputs=[inputs], outputs=[conv10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create dice coefficient\n",
    "def dice_coef(y_true, y_pred, smooth=100):\n",
    "    y_true_flatten = K.flatten(y_true)\n",
    "    y_pred_flatten = K.flatten(y_pred)\n",
    "\n",
    "    intersection = K.sum(y_true_flatten * y_pred_flatten)\n",
    "    union = K.sum(y_true_flatten) + K.sum(y_pred_flatten)\n",
    "    return (2 * intersection + smooth) / (union + smooth)\n",
    "\n",
    "# function to create dice loss\n",
    "def dice_loss(y_true, y_pred, smooth=100):\n",
    "    return -dice_coef(y_true, y_pred, smooth)\n",
    "\n",
    "# function to create iou coefficient\n",
    "def iou_coef(y_true, y_pred, smooth=100):\n",
    "    intersection = K.sum(y_true * y_pred)\n",
    "    sum = K.sum(y_true + y_pred)\n",
    "    iou = (intersection + smooth) / (sum - intersection + smooth)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, masks):\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i in range(25):\n",
    "        plt.subplot(5, 5, i+1)\n",
    "        img_path = images[i]\n",
    "        mask_path = masks[i]\n",
    "        # read image and convert it to RGB scale\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # read mask\n",
    "        mask = cv2.imread(mask_path)\n",
    "        # sho image and mask\n",
    "        plt.imshow(image)\n",
    "        plt.imshow(mask, alpha=0.4)\n",
    "\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'lgg-mri-segmentation/kaggle_3m'\n",
    "\n",
    "df = create_df(data_dir)\n",
    "train_df, valid_df, test_df = split_df(df)\n",
    "\n",
    "\n",
    "tr_aug_dict = dict(rotation_range=0.2,\n",
    "                            width_shift_range=0.05,\n",
    "                            height_shift_range=0.05,\n",
    "                            shear_range=0.05,\n",
    "                            zoom_range=0.05,\n",
    "                            horizontal_flip=True,\n",
    "                            fill_mode='nearest')\n",
    "\n",
    "\n",
    "train_gen = create_gens(train_df, aug_dict=tr_aug_dict)\n",
    "valid_gen = create_gens(valid_df, aug_dict={})\n",
    "test_gen = create_gens(test_df, aug_dict={})\n",
    "\n",
    "show_images(list(train_df['images_paths']), list(train_df['masks_paths']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet()\n",
    "model.compile(Adamax(learning_rate= 0.001), loss= dice_loss, metrics= ['accuracy', iou_coef, dice_coef])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 120\n",
    "batch_size = 40\n",
    "callbacks = [ModelCheckpoint('unet.keras', verbose=0, save_best_only=True)]\n",
    "\n",
    "history = model.fit(train_gen,\n",
    "                    steps_per_epoch=len(train_df) // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data = valid_gen,\n",
    "                    validation_steps=len(valid_df) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_length = len(test_df)\n",
    "test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\n",
    "test_steps = ts_length // test_batch_size\n",
    "\n",
    "train_score = model.evaluate(train_gen, steps= test_steps, verbose= 1)\n",
    "valid_score = model.evaluate(valid_gen, steps= test_steps, verbose= 1)\n",
    "test_score = model.evaluate(test_gen, steps= test_steps, verbose= 1)\n",
    "\n",
    "\n",
    "print(\"Train Loss: \", train_score[0])\n",
    "print(\"Train Accuracy: \", train_score[1])\n",
    "print(\"Train IoU: \", train_score[2])\n",
    "print(\"Train Dice: \", train_score[3])\n",
    "print('-' * 20)\n",
    "\n",
    "print(\"Valid Loss: \", valid_score[0])\n",
    "print(\"Valid Accuracy: \", valid_score[1])\n",
    "print(\"Valid IoU: \", valid_score[2])\n",
    "print(\"Valid Dice: \", valid_score[3])\n",
    "print('-' * 20)\n",
    "\n",
    "print(\"Test Loss: \", test_score[0])\n",
    "print(\"Test Accuracy: \", test_score[1])\n",
    "print(\"Test IoU: \", test_score[2])\n",
    "print(\"Test Dice: \", test_score[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
