{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules loaded\n"
     ]
    }
   ],
   "source": [
    "# import system libs\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pathlib\n",
    "import itertools\n",
    "from glob import glob\n",
    "\n",
    "# import data handling tools\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.morphology import label\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "\n",
    "# import Deep learning Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model, load_model, save_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input, Activation, BatchNormalization, Dropout, Lambda, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, SeparableConv2D, LeakyReLU, UpSampling2D\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print ('modules loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create dataframe\n",
    "def create_df(data_dir):\n",
    "    images_paths = []\n",
    "    masks_paths = glob(f'{data_dir}/*/*_mask*')\n",
    "\n",
    "    for i in masks_paths:\n",
    "        images_paths.append(i.replace('_mask', ''))\n",
    "\n",
    "    df = pd.DataFrame(data= {'images_paths': images_paths, 'masks_paths': masks_paths})\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to split dataframe into train, valid, test\n",
    "def split_df(df):\n",
    "    # create train_df\n",
    "    train_df, dummy_df = train_test_split(df, train_size= 0.8)\n",
    "\n",
    "    # create valid_df and test_df\n",
    "    valid_df, test_df = train_test_split(dummy_df, train_size= 0.5)\n",
    "\n",
    "    return train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gens(df, aug_dict, preprocessing_steps=[]):\n",
    "    img_size = (256, 256)\n",
    "    batch_size = 40\n",
    "\n",
    "    img_gen = ImageDataGenerator(**aug_dict)\n",
    "    msk_gen = ImageDataGenerator(**aug_dict)\n",
    "\n",
    "    image_gen = img_gen.flow_from_dataframe(df, x_col='images_paths', class_mode=None, color_mode='rgb', \n",
    "                                            target_size=img_size, batch_size=batch_size, seed=1)\n",
    "\n",
    "    mask_gen = msk_gen.flow_from_dataframe(df, x_col='masks_paths', class_mode=None, color_mode='grayscale', \n",
    "                                           target_size=img_size, batch_size=batch_size, seed=1)\n",
    "\n",
    "    gen = zip(image_gen, mask_gen)\n",
    "\n",
    "    for img, msk in gen:\n",
    "        img = img / 255.0  # Normalize image to [0, 1]\n",
    "        msk = msk / 255.0  # Normalize mask to [0, 1]\n",
    "        msk[msk > 0.5] = 1  # Binarize mask\n",
    "        msk[msk <= 0.5] = 0  \n",
    "\n",
    "        # Apply preprocessing steps to each image in batch\n",
    "        for step in preprocessing_steps:\n",
    "            img, msk = step(img, msk)\n",
    "\n",
    "        yield (img, msk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score normalization\n",
    "def z_score_norm(img, msk, mean=0.5, std=0.2): \n",
    "    img = (img - mean) / std  \n",
    "    return img, msk\n",
    "\n",
    "# Histogram Equalization (for contrast enhancement)\n",
    "def hist_eq(img, msk):\n",
    "    img_new = np.zeros_like(img)\n",
    "    for i in range(img.shape[0]):  # Loop through batch\n",
    "        img_yuv = cv2.cvtColor((img[i] * 255).astype(np.uint8), cv2.COLOR_RGB2YUV)\n",
    "        img_yuv[:, :, 0] = cv2.equalizeHist(img_yuv[:, :, 0])  # Apply to Y channel\n",
    "        img_new[i] = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2RGB) / 255.0\n",
    "    return img_new, msk\n",
    "\n",
    "\n",
    "# Gaussian Blur (for noise reduction)\n",
    "def gaussian_blur(img, msk, kernel_size=(5,5)):\n",
    "    img = np.array([cv2.GaussianBlur(i, kernel_size, 0) for i in img])\n",
    "    return img, msk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet(input_size=(256, 256, 3)):\n",
    "    inputs = Input(input_size)\n",
    "\n",
    "    # Encoder Path\n",
    "    conv1 = Conv2D(64, (3, 3), padding=\"same\")(inputs)\n",
    "    conv1 = Activation(\"relu\")(conv1)\n",
    "    conv1 = Conv2D(64, (3, 3), padding=\"same\")(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation(\"relu\")(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(128, (3, 3), padding=\"same\")(pool1)\n",
    "    conv2 = Activation(\"relu\")(conv2)\n",
    "    conv2 = Conv2D(128, (3, 3), padding=\"same\")(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation(\"relu\")(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(256, (3, 3), padding=\"same\")(pool2)\n",
    "    conv3 = Activation(\"relu\")(conv3)\n",
    "    conv3 = Conv2D(256, (3, 3), padding=\"same\")(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Activation(\"relu\")(conv3)\n",
    "    pool3 = MaxPooling2D((2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(512, (3, 3), padding=\"same\")(pool3)\n",
    "    conv4 = Activation(\"relu\")(conv4)\n",
    "    conv4 = Conv2D(512, (3, 3), padding=\"same\")(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Activation(\"relu\")(conv4)\n",
    "\n",
    "    # Decoder Path\n",
    "    up5 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding=\"same\")(conv4)\n",
    "    up5 = concatenate([up5, conv3])\n",
    "    conv5 = Conv2D(256, (3, 3), padding=\"same\")(up5)\n",
    "    conv5 = Activation(\"relu\")(conv5)\n",
    "    conv5 = Conv2D(256, (3, 3), padding=\"same\")(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Activation(\"relu\")(conv5)\n",
    "\n",
    "    up6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding=\"same\")(conv5)\n",
    "    up6 = concatenate([up6, conv2])\n",
    "    conv6 = Conv2D(128, (3, 3), padding=\"same\")(up6)\n",
    "    conv6 = Activation(\"relu\")(conv6)\n",
    "    conv6 = Conv2D(128, (3, 3), padding=\"same\")(conv6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Activation(\"relu\")(conv6)\n",
    "\n",
    "    up7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding=\"same\")(conv6)\n",
    "    up7 = concatenate([up7, conv1])\n",
    "    conv7 = Conv2D(64, (3, 3), padding=\"same\")(up7)\n",
    "    conv7 = Activation(\"relu\")(conv7)\n",
    "    conv7 = Conv2D(64, (3, 3), padding=\"same\")(conv7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Activation(\"relu\")(conv7)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation=\"sigmoid\")(conv7)\n",
    "\n",
    "    return Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create dice coefficient\n",
    "def dice_coef(y_true, y_pred, smooth=100):\n",
    "    y_true_flatten = K.flatten(y_true)\n",
    "    y_pred_flatten = K.flatten(y_pred)\n",
    "\n",
    "    intersection = K.sum(y_true_flatten * y_pred_flatten)\n",
    "    union = K.sum(y_true_flatten) + K.sum(y_pred_flatten)\n",
    "    return (2 * intersection + smooth) / (union + smooth)\n",
    "\n",
    "# function to create dice loss\n",
    "def dice_loss(y_true, y_pred, smooth=100):\n",
    "    return -dice_coef(y_true, y_pred, smooth)\n",
    "\n",
    "# function to create iou coefficient\n",
    "def iou_coef(y_true, y_pred, smooth=100):\n",
    "    intersection = K.sum(y_true * y_pred)\n",
    "    sum = K.sum(y_true + y_pred)\n",
    "    iou = (intersection + smooth) / (sum - intersection + smooth)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, masks):\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i in range(25):\n",
    "        plt.subplot(5, 5, i+1)\n",
    "        img_path = images[i]\n",
    "        mask_path = masks[i]\n",
    "        # read image and convert it to RGB scale\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # read mask\n",
    "        mask = cv2.imread(mask_path)\n",
    "        # sho image and mask\n",
    "        plt.imshow(image)\n",
    "        plt.imshow(mask, alpha=0.4)\n",
    "\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'lgg-mri-segmentation/kaggle_3m'\n",
    "\n",
    "df = create_df(data_dir)\n",
    "train_df, valid_df, test_df = split_df(df.head(2000))\n",
    "\n",
    "print(len(train_df))\n",
    "\n",
    "tr_aug_dict = dict(rotation_range=0.2,\n",
    "                            width_shift_range=0.05,\n",
    "                            height_shift_range=0.05,\n",
    "                            shear_range=0.05,\n",
    "                            zoom_range=0.05,\n",
    "                            horizontal_flip=True,\n",
    "                            fill_mode='nearest')\n",
    "\n",
    "\n",
    "preprocessing_pipeline = [z_score_norm, hist_eq, gaussian_blur]\n",
    "\n",
    "train_gen = create_gens(train_df, aug_dict=tr_aug_dict, preprocessing_steps=preprocessing_pipeline)\n",
    "valid_gen = create_gens(valid_df, aug_dict={})\n",
    "test_gen = create_gens(test_df, aug_dict={})\n",
    "\n",
    "#show_images(list(train_df['images_paths']), list(train_df['masks_paths']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet()\n",
    "model.compile(Adamax(learning_rate=0.001), loss=dice_loss, metrics=[iou_coef, dice_coef, \"accuracy\"])\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1600 validated image filenames.\n",
      "Found 1600 validated image filenames.\n",
      "Epoch 1/120\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43s/step - accuracy: 0.7565 - dice_coef: 0.0547 - iou_coef: 0.0284 - loss: -0.0547 Found 200 validated image filenames.\n",
      "Found 200 validated image filenames.\n",
      "\n",
      "Epoch 1: val_loss improved from inf to -0.01670, saving model to unet.keras\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m887s\u001b[0m 44s/step - accuracy: 0.7596 - dice_coef: 0.0553 - iou_coef: 0.0288 - loss: -0.0553 - val_accuracy: 0.9902 - val_dice_coef: 0.0167 - val_iou_coef: 0.0085 - val_loss: -0.0167 - learning_rate: 0.0010\n",
      "Epoch 2/120\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44s/step - accuracy: 0.9033 - dice_coef: 0.0969 - iou_coef: 0.0512 - loss: -0.0969 \n",
      "Epoch 2: val_loss improved from -0.01670 to -0.02052, saving model to unet.keras\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m891s\u001b[0m 45s/step - accuracy: 0.9037 - dice_coef: 0.0971 - iou_coef: 0.0513 - loss: -0.0971 - val_accuracy: 0.1026 - val_dice_coef: 0.0205 - val_iou_coef: 0.0104 - val_loss: -0.0205 - learning_rate: 0.0010\n",
      "Epoch 3/120\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46s/step - accuracy: 0.9337 - dice_coef: 0.1232 - iou_coef: 0.0661 - loss: -0.1232 \n",
      "Epoch 3: val_loss did not improve from -0.02052\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m940s\u001b[0m 47s/step - accuracy: 0.9340 - dice_coef: 0.1237 - iou_coef: 0.0664 - loss: -0.1237 - val_accuracy: 0.9776 - val_dice_coef: 0.0198 - val_iou_coef: 0.0101 - val_loss: -0.0198 - learning_rate: 0.0010\n",
      "Epoch 4/120\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47s/step - accuracy: 0.9522 - dice_coef: 0.1635 - iou_coef: 0.0899 - loss: -0.1635 \n",
      "Epoch 4: val_loss did not improve from -0.02052\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m964s\u001b[0m 48s/step - accuracy: 0.9526 - dice_coef: 0.1639 - iou_coef: 0.0901 - loss: -0.1639 - val_accuracy: 0.9902 - val_dice_coef: 0.0176 - val_iou_coef: 0.0091 - val_loss: -0.0176 - learning_rate: 0.0010\n",
      "Epoch 5/120\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46s/step - accuracy: 0.9711 - dice_coef: 0.1854 - iou_coef: 0.1040 - loss: -0.1854 \n",
      "Epoch 5: val_loss did not improve from -0.02052\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m929s\u001b[0m 46s/step - accuracy: 0.9710 - dice_coef: 0.1856 - iou_coef: 0.1041 - loss: -0.1856 - val_accuracy: 0.9902 - val_dice_coef: 0.0157 - val_iou_coef: 0.0083 - val_loss: -0.0157 - learning_rate: 0.0010\n",
      "Epoch 6/120\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46s/step - accuracy: 0.9727 - dice_coef: 0.2378 - iou_coef: 0.1369 - loss: -0.2378 \n",
      "Epoch 6: val_loss did not improve from -0.02052\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m927s\u001b[0m 46s/step - accuracy: 0.9727 - dice_coef: 0.2379 - iou_coef: 0.1370 - loss: -0.2379 - val_accuracy: 0.9902 - val_dice_coef: 0.0159 - val_iou_coef: 0.0084 - val_loss: -0.0159 - learning_rate: 5.0000e-04\n",
      "Epoch 7/120\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45s/step - accuracy: 0.9823 - dice_coef: 0.2756 - iou_coef: 0.1616 - loss: -0.2756 \n",
      "Epoch 7: val_loss did not improve from -0.02052\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m914s\u001b[0m 46s/step - accuracy: 0.9821 - dice_coef: 0.2753 - iou_coef: 0.1615 - loss: -0.2753 - val_accuracy: 0.9902 - val_dice_coef: 0.0150 - val_iou_coef: 0.0081 - val_loss: -0.0150 - learning_rate: 5.0000e-04\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n"
     ]
    }
   ],
   "source": [
    "epochs = 120\n",
    "batch_size = 80\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint('unet.keras', verbose=1, save_best_only=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n",
    "    EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    steps_per_epoch=len(train_df) // batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=valid_gen,\n",
    "    validation_steps=len(valid_df) // batch_size,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 7s/step - accuracy: 0.1555 - dice_coef: 0.0208 - iou_coef: 0.0105 - loss: -0.0208\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7s/step - accuracy: 0.0978 - dice_coef: 0.0106 - iou_coef: 0.0053 - loss: -0.0106\n",
      "Found 200 validated image filenames.\n",
      "Found 200 validated image filenames.\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7s/step - accuracy: 0.1056 - dice_coef: 0.0262 - iou_coef: 0.0133 - loss: -0.0262\n",
      "Train Loss:  -0.020716547966003418\n",
      "Train IoU:  0.010497646406292915\n",
      "Train Dice:  0.020716547966003418\n",
      "Train Accuracy:  0.154937744140625\n",
      "--------------------\n",
      "Valid Loss:  -0.011254568584263325\n",
      "Valid IoU:  0.005689137615263462\n",
      "Valid Dice:  0.011254568584263325\n",
      "Valid Accuracy:  0.09804725646972656\n",
      "--------------------\n",
      "Test Loss:  -0.023139238357543945\n",
      "Test IoU:  0.011748242191970348\n",
      "Test Dice:  0.023139238357543945\n",
      "Test Accuracy:  0.10415935516357422\n"
     ]
    }
   ],
   "source": [
    "ts_length = len(test_df)\n",
    "test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\n",
    "test_steps = ts_length // test_batch_size\n",
    "\n",
    "train_score = model.evaluate(train_gen, steps= test_steps, verbose= 1)\n",
    "valid_score = model.evaluate(valid_gen, steps= test_steps, verbose= 1)\n",
    "test_score = model.evaluate(test_gen, steps= test_steps, verbose= 1)\n",
    "\n",
    "\n",
    "print(\"Train Loss: \", train_score[0])\n",
    "print(\"Train IoU: \", train_score[1])\n",
    "print(\"Train Dice: \", train_score[2])\n",
    "print(\"Train Accuracy: \", train_score[3])\n",
    "print('-' * 20)\n",
    "\n",
    "print(\"Valid Loss: \", valid_score[0])\n",
    "print(\"Valid IoU: \", valid_score[1])\n",
    "print(\"Valid Dice: \", valid_score[2])\n",
    "print(\"Valid Accuracy: \", valid_score[3])\n",
    "print('-' * 20)\n",
    "\n",
    "print(\"Test Loss: \", test_score[0])\n",
    "print(\"Test IoU: \", test_score[1])\n",
    "print(\"Test Dice: \", test_score[2])\n",
    "print(\"Test Accuracy: \", test_score[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
